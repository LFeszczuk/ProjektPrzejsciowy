{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import tempfile\n",
    "import os\n",
    "from main import data_extract\n",
    "# Preset Matplotlib figure sizes.\n",
    "matplotlib.rcParams['figure.figsize'] = [9, 6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Denis\\.pyenv\\pyenv-win\\versions\\3.8.9\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "print(tf.__version__)\n",
    "# Set random seed for reproducible results \n",
    "tf.random.set_seed(22)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_extract' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data_extract()[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_extract' is not defined"
     ]
    }
   ],
   "source": [
    "data_extract()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = tfds.load(\"mnist\", \n",
    "                                            split=['train[10000:]', 'train[0:10000]', 'test'],\n",
    "                                            batch_size=128, as_supervised=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_viz, y_viz = tfds.load(\"mnist\", split=['train[:1500]'], batch_size=-1, as_supervised=True)[0]\n",
    "x_viz = tf.squeeze(x_viz, axis=3)\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,1+i)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(x_viz[i], cmap='gray')\n",
    "    plt.title(f\"True Label: {y_viz[i]}\")\n",
    "    plt.subplots_adjust(hspace=.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(y_viz.numpy());\n",
    "plt.xlabel('Digits')\n",
    "plt.title(\"MNIST Digit Distribution\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "  # Reshaping the data\n",
    "  x = tf.reshape(x, shape=[-1, 784])\n",
    "  # Rescaling the data\n",
    "  x = x/255\n",
    "  return x, y\n",
    "\n",
    "train_data, val_data = train_data.map(preprocess), val_data.map(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.linspace(-2, 2, 201)\n",
    "x = tf.cast(x, tf.float32)\n",
    "plt.plot(x, tf.nn.relu(x));\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('ReLU(x)')\n",
    "plt.title('ReLU activation function');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.linspace(-4, 4, 201)\n",
    "x = tf.cast(x, tf.float32)\n",
    "plt.plot(x, tf.nn.softmax(x, axis=0));\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Softmax(x)')\n",
    "plt.title('Softmax activation function');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(shape):\n",
    "  # Computes the xavier initialization values for a weight matrix\n",
    "  in_dim, out_dim = shape\n",
    "  xavier_lim = tf.sqrt(6.)/tf.sqrt(tf.cast(in_dim + out_dim, tf.float32))\n",
    "  weight_vals = tf.random.uniform(shape=(in_dim, out_dim), \n",
    "                                  minval=-xavier_lim, maxval=xavier_lim, seed=22)\n",
    "  return weight_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(tf.Module):\n",
    "\n",
    "  def __init__(self, out_dim, weight_init=xavier_init, activation=tf.identity):\n",
    "    # Initialize the dimensions and activation functions\n",
    "    self.out_dim = out_dim\n",
    "    self.weight_init = weight_init\n",
    "    self.activation = activation\n",
    "    self.built = False\n",
    "\n",
    "  def __call__(self, x):\n",
    "    if not self.built:\n",
    "      # Infer the input dimension based on first call\n",
    "      self.in_dim = x.shape[1]\n",
    "      # Initialize the weights and biases using Xavier scheme\n",
    "      self.w = tf.Variable(xavier_init(shape=(self.in_dim, self.out_dim)))\n",
    "      self.b = tf.Variable(tf.zeros(shape=(self.out_dim,)))\n",
    "      self.built = True\n",
    "    # Compute the forward pass\n",
    "    z = tf.add(tf.matmul(x, self.w), self.b)\n",
    "    return self.activation(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.Module):\n",
    "\n",
    "  def __init__(self, layers):\n",
    "    self.layers = layers\n",
    "\n",
    "  @tf.function\n",
    "  def __call__(self, x, preds=False): \n",
    "    # Execute the model's layers sequentially\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_1_size = 700\n",
    "hidden_layer_2_size = 500\n",
    "output_size = 10\n",
    "\n",
    "mlp_model = MLP([\n",
    "    DenseLayer(out_dim=hidden_layer_1_size, activation=tf.nn.relu),\n",
    "    DenseLayer(out_dim=hidden_layer_2_size, activation=tf.nn.relu),\n",
    "    DenseLayer(out_dim=output_size)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y):\n",
    "  # Compute cross entropy loss with a sparse operation\n",
    "  sparse_ce = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=y_pred)\n",
    "  return tf.reduce_mean(sparse_ce)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y):\n",
    "  # Compute accuracy after extracting class predictions\n",
    "  class_preds = tf.argmax(tf.nn.softmax(y_pred), axis=1)\n",
    "  is_equal = tf.equal(y, class_preds)\n",
    "  return tf.reduce_mean(tf.cast(is_equal, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "\n",
    "    def __init__(self, learning_rate=1e-3, beta_1=0.9, beta_2=0.999, ep=1e-7):\n",
    "      # Initialize optimizer parameters and variable slots\n",
    "      self.beta_1 = beta_1\n",
    "      self.beta_2 = beta_2\n",
    "      self.learning_rate = learning_rate\n",
    "      self.ep = ep\n",
    "      self.t = 1.\n",
    "      self.v_dvar, self.s_dvar = [], []\n",
    "      self.built = False\n",
    "\n",
    "    def apply_gradients(self, grads, vars):\n",
    "      # Initialize variables on the first call\n",
    "      if not self.built:\n",
    "        for var in vars:\n",
    "          v = tf.Variable(tf.zeros(shape=var.shape))\n",
    "          s = tf.Variable(tf.zeros(shape=var.shape))\n",
    "          self.v_dvar.append(v)\n",
    "          self.s_dvar.append(s)\n",
    "        self.built = True\n",
    "      # Update the model variables given their gradients\n",
    "      for i, (d_var, var) in enumerate(zip(grads, vars)):\n",
    "        self.v_dvar[i].assign(self.beta_1*self.v_dvar[i] + (1-self.beta_1)*d_var)\n",
    "        self.s_dvar[i].assign(self.beta_2*self.s_dvar[i] + (1-self.beta_2)*tf.square(d_var))\n",
    "        v_dvar_bc = self.v_dvar[i]/(1-(self.beta_1**self.t))\n",
    "        s_dvar_bc = self.s_dvar[i]/(1-(self.beta_2**self.t))\n",
    "        var.assign_sub(self.learning_rate*(v_dvar_bc/(tf.sqrt(s_dvar_bc) + self.ep)))\n",
    "      self.t += 1.\n",
    "      return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_batch, y_batch, loss, acc, model, optimizer):\n",
    "  # Update the model state given a batch of data\n",
    "  with tf.GradientTape() as tape:\n",
    "    y_pred = model(x_batch)\n",
    "    batch_loss = loss(y_pred, y_batch)\n",
    "  batch_acc = acc(y_pred, y_batch)\n",
    "  grads = tape.gradient(batch_loss, model.variables)\n",
    "  optimizer.apply_gradients(grads, model.variables)\n",
    "  return batch_loss, batch_acc\n",
    "\n",
    "def val_step(x_batch, y_batch, loss, acc, model):\n",
    "  # Evaluate the model on given a batch of validation data\n",
    "  y_pred = model(x_batch)\n",
    "  batch_loss = loss(y_pred, y_batch)\n",
    "  batch_acc = acc(y_pred, y_batch)\n",
    "  return batch_loss, batch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(mlp, train_data, val_data, loss, acc, optimizer, epochs):\n",
    "  # Initialize data structures\n",
    "  train_losses, train_accs = [], []\n",
    "  val_losses, val_accs = [], []\n",
    "\n",
    "  # Format training loop and begin training\n",
    "  for epoch in range(epochs):\n",
    "    batch_losses_train, batch_accs_train = [], []\n",
    "    batch_losses_val, batch_accs_val = [], []\n",
    "\n",
    "    # Iterate over the training data\n",
    "    for x_batch, y_batch in train_data:\n",
    "      # Compute gradients and update the model's parameters\n",
    "      batch_loss, batch_acc = train_step(x_batch, y_batch, loss, acc, mlp, optimizer)\n",
    "      # Keep track of batch-level training performance\n",
    "      batch_losses_train.append(batch_loss)\n",
    "      batch_accs_train.append(batch_acc)\n",
    "\n",
    "    # Iterate over the validation data\n",
    "    for x_batch, y_batch in val_data:\n",
    "      batch_loss, batch_acc = val_step(x_batch, y_batch, loss, acc, mlp)\n",
    "      batch_losses_val.append(batch_loss)\n",
    "      batch_accs_val.append(batch_acc)\n",
    "\n",
    "    # Keep track of epoch-level model performance\n",
    "    train_loss, train_acc = tf.reduce_mean(batch_losses_train), tf.reduce_mean(batch_accs_train)\n",
    "    val_loss, val_acc = tf.reduce_mean(batch_losses_val), tf.reduce_mean(batch_accs_val)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    print(f\"Training loss: {train_loss:.3f}, Training accuracy: {train_acc:.3f}\")\n",
    "    print(f\"Validation loss: {val_loss:.3f}, Validation accuracy: {val_acc:.3f}\")\n",
    "  return train_losses, train_accs, val_losses, val_accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_accs, val_losses, val_accs = train_model(mlp_model, train_data, val_data, \n",
    "                                                             loss=cross_entropy_loss, acc=accuracy,\n",
    "                                                             optimizer=Adam(), epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(train_metric, val_metric, metric_type):\n",
    "  # Visualize metrics vs training Epochs\n",
    "  plt.figure()\n",
    "  plt.plot(range(len(train_metric)), train_metric, label = f\"Training {metric_type}\")\n",
    "  plt.plot(range(len(val_metric)), val_metric, label = f\"Validation {metric_type}\")\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric_type)\n",
    "  plt.legend()\n",
    "  plt.title(f\"{metric_type} vs Training epochs\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExportModule(tf.Module):\n",
    "  def __init__(self, model, preprocess, class_pred):\n",
    "    # Initialize pre and postprocessing functions\n",
    "    self.model = model\n",
    "    self.preprocess = preprocess\n",
    "    self.class_pred = class_pred\n",
    "\n",
    "  @tf.function(input_signature=[tf.TensorSpec(shape=[None, None, None, None], dtype=tf.uint8)]) \n",
    "  def __call__(self, x):\n",
    "    # Run the ExportModule for new data points\n",
    "    x = self.preprocess(x)\n",
    "    y = self.model(x)\n",
    "    y = self.class_pred(y)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test(x):\n",
    "  # The export module takes in unprocessed and unlabeled data\n",
    "  x = tf.reshape(x, shape=[-1, 784])\n",
    "  x = x/255\n",
    "  return x\n",
    "\n",
    "def class_pred_test(y):\n",
    "  # Generate class predictions from MLP output\n",
    "  return tf.argmax(tf.nn.softmax(y), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model_export = ExportModule(model=mlp_model,\n",
    "                                preprocess=preprocess_test,\n",
    "                                class_pred=class_pred_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = tempfile.mkdtemp()\n",
    "save_path = os.path.join(models, 'mlp_model_export')\n",
    "tf.saved_model.save(mlp_model_export, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_loaded = tf.saved_model.load(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y_pred, y):\n",
    "  # Generic accuracy function\n",
    "  is_equal = tf.equal(y_pred, y)\n",
    "  return tf.reduce_mean(tf.cast(is_equal, tf.float32))\n",
    "\n",
    "x_test, y_test = tfds.load(\"mnist\", split=['test'], batch_size=-1, as_supervised=True)[0]\n",
    "test_classes = mlp_loaded(x_test)\n",
    "test_acc = accuracy_score(test_classes, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy breakdown by digit:\")\n",
    "print(\"---------------------------\")\n",
    "label_accs = {}\n",
    "for label in range(10):\n",
    "  label_ind = (y_test == label)\n",
    "  # extract predictions for specific true label\n",
    "  pred_label = test_classes[label_ind]\n",
    "  label_filled = tf.cast(tf.fill(pred_label.shape[0], label), tf.int64)\n",
    "  # compute class-wise accuracy\n",
    "  label_accs[accuracy_score(pred_label, label_filled).numpy()] = label\n",
    "for key in sorted(label_accs):\n",
    "  print(f\"Digit {label_accs[key]}: {key:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as sk_metrics\n",
    "\n",
    "def show_confusion_matrix(test_labels, test_classes):\n",
    "  # Compute confusion matrix and normalize\n",
    "  plt.figure(figsize=(10,10))\n",
    "  confusion = sk_metrics.confusion_matrix(test_labels.numpy(), \n",
    "                                          test_classes.numpy())\n",
    "  confusion_normalized = confusion / confusion.sum(axis=1)\n",
    "  axis_labels = range(10)\n",
    "  ax = sns.heatmap(\n",
    "      confusion_normalized, xticklabels=axis_labels, yticklabels=axis_labels,\n",
    "      cmap='Blues', annot=True, fmt='.4f', square=True)\n",
    "  plt.title(\"Confusion matrix\")\n",
    "  plt.ylabel(\"True label\")\n",
    "  plt.xlabel(\"Predicted label\")\n",
    "\n",
    "show_confusion_matrix(y_test, test_classes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b6db4e962777f90657782466d51e49b62fd835a4bfde134cf675f8a3693fa296"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
